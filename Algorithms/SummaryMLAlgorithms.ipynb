{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <span style='color:black'> Machine Learning: </span>\n",
    "A method to teach computers to learn from data.\n",
    "- Supervised: the computer is provided with labeled data. The goal is to learn a general rule that maps inputs to outputs.\n",
    "    - Image classification, linear regression, spam detection.\n",
    "- Unsupervised: No labeled data. The goal is to find patterns or relations in data.\n",
    "    - Clustering, Anomaly detection, dimensionality detection.\n",
    "- Reinforcement learning: Computers interact with an environment and learn to make a decision based on the response of the environment and rewards and penalties.\n",
    "\n",
    "ML algorithms can also be divided into two categories\n",
    "- **Probabilistic**\n",
    "    - These are based on the idea of estimating the underlying probability distribution of the data, and using this distribution to make predictions. These models typically use Bayesian methods, which involve updating the probability distribution as new data becomes available. Examples of probabilistic ML models include Gaussian processes, hidden Markov models, and Bayesian networks.\n",
    "- **Statistical**\n",
    "    - These models are based on the idea of finding the best fixed model that explains the data, given a set of assumptions and a criterion for model selection. These models typically use frequentist methods, which involve optimizing the parameters of the model using maximum likelihood estimation or maximum a posteriori estimation. Examples of statistical ML models include linear regression, logistic regression, and support vector machines.\n",
    "\n",
    "In summary, probabilistic models estimate probability distributions, while statistical models find the best fixed model that explains the data, given a set of assumptions.\n",
    "\n",
    "\n",
    "\n",
    "# <span style='color:black'> Deep Learning: </span>:\n",
    "It is a subfield of machine learning based on the use of neural networks (NN) and their development. Interacting layers in NN are able to learn and make decisions after extracting features from data.\n",
    "- Natural language processing\n",
    "- computer vision, image recognition\n",
    "- speech recognition.\n",
    "\n",
    "\n",
    "Machine learning algorithms are divided into two main categories, **parametric** and **non-parametric**. In the parametric category, algorithms make assumptions about the form of the relation between inputs and outputs. For example in linear regression, it is assumed that there is a linear relation between features and corresponding target. However, there is not any assumption for non-parametric algorithms such as KNN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <span style='color:blue'> K-nearest neighbor (KNN) </span>\n",
    "\n",
    "* A **non-parametric** model\n",
    "* A **non-linear** model\n",
    "\n",
    "It makes few assumptions about structure of data and usually gives accurate result, but it is unstable to small changes in the dataset.\n",
    "\n",
    "* Classifier\n",
    "* Regressor\n",
    "\n",
    "Instance based or memory based supervised learning. It is computationally expensive when the dataset is large.\n",
    "\n",
    "- **KNN classifier**: memorize the entire training set.\n",
    "\n",
    "\n",
    "Four things should be specified:\n",
    "\n",
    "    1) A distance metric.\n",
    "        * it controls the distance function between points and thus which points are considered as nearest in finding neighbors.\n",
    "        * Typically Euclidean (Minkowski with p = 2)\n",
    "    2) How many nearest neighbors to look at? (Model complexity)\n",
    "        * k=5\n",
    "    3) Optional weighting function on the neighbor points.\n",
    "        * Ignored\n",
    "    4) Methods for aggregating the classes of neighbor points.\n",
    "        * Majority class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "9ca618c9",
   "metadata": {},
   "source": [
    "### Relation between $k$ and model complexity\n",
    "\n",
    "* **Reducing $k$** in knn classifier **increases** the variance of the decision boundries and the risk of **Overfitting** because very local changes is captured.\n",
    "\n",
    "* **$k=$the total number** of points in the training set, the result would be a **single decision** which it is the **most frequent** calss in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d319428",
   "metadata": {},
   "source": [
    "### Drawback:\n",
    "\n",
    "When the training data has many samples, or each sample has lots of features, this can slow down the performance of KNN model.\n",
    "\n",
    "For data set with hundred of thousands of features, especially if it is sparse, we should apply another model in stead of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fdb4c4",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>  Linear regression: </span>\n",
    "\n",
    "Linear regression is a **parametric** statistical algorithm assuming a linear relationship between inputs and outputs.  It gives the target based on weighted sum of the features where the goal is to find the best line of fit to minimize the difference between predicted and actual values.\n",
    "\n",
    "There are different methods to find the optimal values for the model, normal equations or optimization algorithms like gradient descent.\n",
    "\n",
    "Linear regression can be extended to handle non-linear relationships by applying a non-linear transformation to the inputs which is called polynomial regression which I will explain below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36427ac0",
   "metadata": {},
   "source": [
    "**Least square linear regression** (AKA: ordinary least square)\n",
    "\n",
    "* It minimize the mean square error between target and prediction to find ws(weights) and b (bias/intercept parameter)\n",
    "\n",
    " $RSS(w,b) = \\sum_{i=1}^N (y_i - (w.x_i + b))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a6959b",
   "metadata": {},
   "source": [
    "### <span style='color:green'>scikit-learn:</span>\n",
    "\n",
    "* $w$ : linreg.coef_\n",
    "\n",
    "* $b$: linreg.intercept_\n",
    "\n",
    "    -The ' _ ' in linreg.coef_ means it is a parameter that has been derived by training the data and it is not set by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378cc17c",
   "metadata": {},
   "source": [
    "## Comparing between KNN and Linear regression:\n",
    "\n",
    "* **KNN**:\n",
    "    - does not make a lot of assumption about the structure of the data.\n",
    "    - gives potentially accurate but sometimes unstable predictions that are sensitive to small changes in the training data.\n",
    "    - better on training set.\n",
    "\n",
    "* **Linear regression**:\n",
    "    - makes strong assuptions about the structure of the data: linear relationship.\n",
    "    - gives stable but potentially inaccurate predictions.\n",
    "    - better on unseen data.\n",
    "    - very extendable to new data beyond the training set.\n",
    "    - no parameter to control the complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d4049",
   "metadata": {},
   "source": [
    "## <span style='color:orange'>  Regularization: </span>\n",
    "Regularization prevents **overfitting** by restricting the model typically to reduce its complexity. \n",
    "\n",
    "* Ridge regression (L2)\n",
    "* Lasso regression (L1)\n",
    "\n",
    "### Ridge regression:\n",
    "\n",
    "It is a type of linear regression that adds a regularization term to the cost function to prevent overfitting. It's useful for the case that some features are correlated. It forces the model to consider only the most important variables for the prediction. This model has a hyperparameter that controls the regularization. It uses the normal equation to find optimal parameters.\n",
    "\n",
    "Using same least-square criterion but adds a penalty for **large variations** in weight parameters.\n",
    "\n",
    "$RSS_{ridge}(w,b) = \\sum_{i=1}^N (y_i - (w.x_i+b))^2 + \\alpha \\sum_{i=1}^P w_j^2$\n",
    "\n",
    "**Higher** $\\alpha$ means **more** regularization and **simpler** models.\n",
    "\n",
    "### Lasso regression:\n",
    "\n",
    "Same as Ridge regression, here, a regularization term is added to the cost function that cause w coefficients to shrink toward zero to prevent overfitting. It uses optimization algorithms such as gradient descent to find optimal parameters. Unlike Ridge regression, Lasso regression tends to give sparse solutions, meaning that it tends to set some parameters to zero, this helps to select the most important features for the prediction.\n",
    "\n",
    "\n",
    "$RSS_{lasso}(w,b) = \\sum_{i=1}^N (y_i - (w.x_i+b))^2 + \\alpha \\sum_{i=1}^P |w_j|$\n",
    "\n",
    "With lasso, a subset of the coefficients are forced to be precisely zero. (it is called sparse solution which is a kind of **Feature selection**)\n",
    "\n",
    "By default $\\alpha=0$.\n",
    "\n",
    "\n",
    "### Use\n",
    "* **Ridge**: Many small/Medium sized effects.\n",
    "* **Lasso**: Only a few variables with medium effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6ec26",
   "metadata": {},
   "source": [
    "# Polynomial Features:\n",
    "\n",
    "Generate polynomial and interaction features.\n",
    "\n",
    "* It is still a **linear** model.\n",
    "* Polynomial feature expansion is often combined with a regularization learning method like ridge regression.\n",
    "* Using higher degrees leads to more complex models and regularization might be needed to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2f36b",
   "metadata": {},
   "source": [
    "# Linear model for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab90293c",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>  Logistic regression </span>\n",
    "\n",
    "It is a **statistical linear method** for binary classification i.e., it gives binary outcomes (0/1). It performs best when the classes are well-balanced and data is not too complex.\n",
    "\n",
    "* Linear model\n",
    "* default: Binary classification but can be applied on multi-class \n",
    "* Applying logistic function (activation function) on estimated probabilities determines the class\n",
    "* Parameter $C$ controls **regularization**\n",
    "    - default: $C = 1$ Ridge (L2) regularization\n",
    "    - **Higher** $C$ corresponds to **less** regularization\n",
    "* Parameter $\\gamma$ is a hyperparameter which we have to set before training model.\n",
    "    -  $\\gamma$ is a parameter for **non-linear** hyperplanes\n",
    "    - $\\gamma$ decides that how much curvature we want in a decision boundary\n",
    "    - **Higher** $\\gamma$ tries to exactly fit the training data set\n",
    "* Normalization would be important here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad58a2f",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>  Support vector machine (SVM): </span>\n",
    "\n",
    "\n",
    "Support vector machine is a supervised algorithm that can be used for\n",
    "- regression,\n",
    " - classification.\n",
    "\n",
    "It finds a hyperplane to separate the data into two different classes. SVM is good when the data has many features and the boundary is not linear. The algorithm uses a technique called the kernel trick to transform the data into a higher dimensional space where a linear boundary can separate the classes.\n",
    "\n",
    "A kernel function is a mathematical function that is used to transform the input data into a higher-dimensional space where it becomes linearly separable. SVM uses a kernel function to handle non-linearly separable data, while LSVM uses a linear kernel function.\n",
    "\n",
    "SVM can use different types of kernel functions, such as polynomial, radial basis function (RBF), and sigmoid, which allow them to handle **non-linearly** separable data more effectively. However, these kernel functions can be computationally expensive, which can make SVMs less efficient for large datasets.\n",
    "\n",
    "\n",
    "The main advantage of using a linear kernel function in LSVM is its simplicity and efficiency, as it doesn't require any additional computation to transform the data into a higher-dimensional space, which makes it computationally efficient for large datasets. Additionally, it's simpler to understand and implement, and it can be regularized to prevent overfitting.\n",
    "\n",
    "* Apply **sign function** as activation function to produce binary output\n",
    "    -feature vector -> linear function: $Sign(w.x+b)$ -> class value\n",
    "* **Classifier margin** is defined as the width the decision boundary area can be increased before hitting a data point.\n",
    "* The **best** classifier has the **maximum** margin.\n",
    "* The **maximum** margin classifier is called the **linear support vector machine (LSVM)**\n",
    "* Parameter $C$ controls **regularization**\n",
    "    - default: $C = 1$ Ridge (L2) regularization\n",
    "    - **Higher** $C$ corresponds to **less** regularization.\n",
    "        * Fit the training data as well as possible\n",
    "        * Each individual data point is important to classify correctly.\n",
    "* Parameter $\\gamma$ is a hyperparameter which we have to set before training model.\n",
    "    -  $\\gamma$ is a parameter for **non-linear** hyperplanes.\n",
    "    - $\\gamma$ decides that how much curvature we want in a decision boundary.\n",
    "    - **Higher** $\\gamma$ tries to exactly fit the training data set\n",
    "* Parameter kernel\n",
    "    - show the type of hyperplane used to separate the data.\n",
    "        * linear hyperplane : ‘linear’ (a line in the case of 2D data). \n",
    "        * non-linear hyperplane: ‘rbf’ and ‘poly’\n",
    "        \n",
    "* **Drawback**:\n",
    "    \n",
    "\n",
    "* **Benefit**:\n",
    "    - Simple and easy to train\n",
    "    - Fast prediction\n",
    "    - Scales well to very large datasets\n",
    "    \n",
    "### <span style='color:green'>scikit-learn</span>:\n",
    "\n",
    "* sklearn.svm.SVC\n",
    "* sklearn.svm.LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5126e17",
   "metadata": {},
   "source": [
    "## Multiclass classifier:\n",
    "\n",
    "* scikit-learn make multi-class problem into binary problems (one class against all other classes).\n",
    "    - choose the class with **highest score** as the **predicted class**.\n",
    "    - model.coef_ gives us $n$ (number of classes) sets of parameters related to each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <span style='color:blue'>  Linear Support vector machine (LSVM): </span>\n",
    "\n",
    "As same as Support Vector Machine,  Linear Support Vector Machine (LSVM) area  supervised learning algorithm that can be used for both classification and regression tasks. However, the main difference between them is the type of kernel function they use.\n",
    "\n",
    "The main advantage of using a linear kernel function in LSVM is its simplicity and efficiency, as it doesn't require any additional computation to transform the data into a higher-dimensional space, which makes it computationally efficient for large datasets. Additionally, it's simpler to understand and implement, and it can be regularized to prevent overfitting.\n",
    " However, it's important to note that the choice of the algorithm depends on the specific problem, and both SVM and LSVM have their own advantages and limitations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "a69b0300",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>  Kernelized Support vector machine (KSVM): </span>\n",
    "\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43e9ab",
   "metadata": {},
   "source": [
    "# <span style='color:orange'>  Train-test split </span>\n",
    "\n",
    "If the data is randomly splitted into training and test sets, the model is trained on the training set and the test set is used for validation purpose, ideally split the data into 70:30 or 80:20. Here, there is a possibility of **high bias** if we have **limited data**, because we would miss some information about the data which we have not used for training.\n",
    "\n",
    "To solve this problem we go for **Cross validation** approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc45f35",
   "metadata": {},
   "source": [
    "# <span style='color:orange'>  Cross validation: </span>\n",
    "\n",
    "How well a classifier generalizes. It results in a **less biased** model.\n",
    "\n",
    "* **why?** The test set represented data that had not been seen during training but had the **same** general attributes as the original data set. The normal evaluation on a single test set has a problem: maybe the result becomes good for that specific testset by chance. So we go for **cross-validation**.\n",
    "\n",
    "* **k-fold cross validation**:\n",
    "cross-validation split the dataset to e.g. k=5 parts then consider 1 part as the testset and the rest as training set and evaluate. Then consider the next part as the test set, etc. At the end we can look into the average result.\n",
    "\n",
    "* The **Stratified Cross-validation** means that when splitting the data, the proportions of classes in each fold are made as close as possible to the actual proportions of the classes in the overall data set as shown here.\n",
    "\n",
    "\n",
    "It has two steps:\n",
    "\n",
    "1) Splitting the data set into subsets (k folds):\n",
    "\n",
    "    - Each fold has approximately the same size\n",
    "    - Data can be randomly selected in each fold or stratified\n",
    "\n",
    "2) Rotating, training and validating among them\n",
    "    \n",
    "    - All folds are used to train the model except one, which is used for validation.\n",
    "    - That validation fold should be rotated until all folds have become a validation fold once and only once.\n",
    "    - Each example is recommended to be contained in one and only one fold.\n",
    "    \n",
    "    \n",
    "## <span style='color:green'>scikit-learn</span>:\n",
    "\n",
    "### Key parameters: \n",
    "\n",
    "* cross_val_score: returns score of each test folds\n",
    "* corss_val_predict: returns the predicted score for each observation in the input dataset when it was part of the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85fd93",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> Decision Tree </span>\n",
    "\n",
    "* Classifier\n",
    "* regression\n",
    "    - the decision tree works like a classifier and then gives the mean value of the result class.\n",
    "\n",
    "Based on a series of **if-then** / **yes-no** questions. Each question is a node. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Different types of nodes:\n",
    "\n",
    "    1) Root node\n",
    "        - Top\n",
    "    2) Decision node\n",
    "        - Middle\n",
    "    3) Terminated node (Leaf)\n",
    "        - bottom\n",
    "\n",
    "### Deawback:\n",
    "\n",
    "**Overfitting** are likely to happen.\n",
    "To prevent overfitting, two additional strategies are used:\n",
    "* pre-pruning\n",
    "    - early stop of growing tree\n",
    "* post-pruning\n",
    "    - ???\n",
    "    \n",
    "### Benefit:\n",
    "* Works well with different types of features (Categorical non-categorical)\n",
    "* No need for normalization\n",
    "    \n",
    "## <span style='color:green'>scikit-learn</span>:\n",
    "\n",
    "### Key parameters: \n",
    "Controling one of the below parameters is enough to control overfitting.\n",
    "\n",
    "* **max_depth**: controls maximum number of split points.\n",
    "    - Most common way to reduce the complexity of the model.\n",
    "* **min_samples_leaf**: threshold for minimum number of data instances a leaf can have to avoid further splitting.\n",
    "* **max_leaf_nodes**: limit total number of leaves in the tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e534bb",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> Random forest </span>\n",
    "\n",
    "To avoid **Overfitting** in Decision tree, it is needed to use an ensemble of trees.\n",
    "* Classifier\n",
    "* Regressor\n",
    "\n",
    "Here in Random forest algorithm,\n",
    "- Data used to build each tree is **randomly** selected\n",
    "- Feature chosen in each split test are also **randomly** selected.\n",
    "\n",
    "## <span style='color:green'>scikit-learn</span>:\n",
    "\n",
    "### Key parameters: \n",
    "\n",
    "* **n_estimated**: the number of estimators shows how many trees considered in the random forest.\n",
    "* **max_features**: the number of features in the subset that are randomly considered at each stage is controlled by it.\n",
    "\n",
    "### Methods:\n",
    "* decision_function(x)\n",
    "    - predicts confidence scores for samples.\n",
    "* predict(x)\n",
    "    - Predict class for X.\n",
    "* predict_proba(X)\n",
    "    - Predict class probabilities for X.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b0526b",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> Naive Bayes Classifiers </span>\n",
    "* Linear classifier\n",
    "* Based on Bayes theorem\n",
    "* Based on simple probabilistic model\n",
    "    - Each feature is independent of all the others.\n",
    "    - All the predictors have an equal effect on the outcome.\n",
    "        * learning is very fast.\n",
    "* Mostly used in sentiment analysis\n",
    "    - spam filtering\n",
    "    - recommendation systems\n",
    "    - \n",
    "        \n",
    "Different types of Bayes Classifier:\n",
    "* Bernoulli:\n",
    "    - Binary features (Boolean variables)\n",
    "        * True/False\n",
    "        * word presence/absence in a text (if a word occurs in the text or not)\n",
    "        * Spam/not spam\n",
    "        * 0/1\n",
    "    - Work well if the dataset is small compared to other classification algorithms in the case of a small dataset.\n",
    "* Multinomial:\n",
    "    - mostly used for document classification problem\n",
    "        * whether a document belongs to the category of sports, politics, technology etc.\n",
    "    - dataset that is distributed multinomially.\n",
    "    - discrete features\n",
    "        * **frequency** of the words present in the document\n",
    "* Gaussian:\n",
    "    - Continuous/real-valued features\n",
    "    - assume that the data for each class was generated by a simple class specific Gaussian distribution.\n",
    "    - Decision boundary is in general a parabolic curve between the clases.\n",
    "    - used for high dimentional data\n",
    "    - partial.fit ????\n",
    "\n",
    "\n",
    "### Deawback:\n",
    "- features are considered as independent values\n",
    "\n",
    "### Benefit:\n",
    "- Easy impelement\n",
    "- Fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb84f78",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> Dummy estimators </span>\n",
    "\n",
    "Dummy estimator is used to obtain a simple baseline to compare with complex algorithm.\n",
    "\n",
    "- Classifier\n",
    "- Regressor\n",
    "\n",
    "\n",
    "**Dummy Classifier** is a classifier model that makes predictions without trying to find patterns in the data.\n",
    "- provides null accuracy baseline\n",
    "- strategy:\n",
    "    * **most_frequent**: predicts the most frequent label in the training data\n",
    "        - default of the Dummy classifier\n",
    "    * **stratified**: predicts random predictions based on training set class distribution\n",
    "    * **uniform**: generate predictions uniformly at random ???\n",
    "    * **constant**: predicts a constant label provided by user\n",
    "\n",
    "**Dummy regressor**\n",
    "\n",
    "- strategy:\n",
    "    * **mean**: predicts the mean of the training set.\n",
    "        - default of the Dummy regressor\n",
    "    * **median**: predicts the median of the training set\n",
    "    * **quantile**: ???\n",
    "    * **constant**: predicts a constant label provided by user\n",
    "    \n",
    "    \n",
    "### Drawback:\n",
    "- Ineffective\n",
    "\n",
    "### Benefit:\n",
    "- gives a baseline of metric\n",
    "- large class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <span style='color:black'> Optimization algorithms </span>\n",
    " The goal of optimization algorithms is to find the set of parameters that minimize the cost function or objective function.\n",
    "\n",
    "- **Normal equation** is a non-iterative method to find an optimal solution for certain types of linear regression. It solves the problem exactly by minimizing the difference between predicted values and actual values. An alternative to solving linear regression is using gradient descent which is an iterative optimization approach.\n",
    "\n",
    "- **Gradient descent** (GD) is an iterative first-order optimization algorithm used to find a local minimum/maximum of a given function. The algorithm works by iteratively moving in the direction of the negative gradient of the function, which is the direction of the steepest descent. The basic idea behind gradient descent is that at each iteration, the algorithm updates the parameters of the model by a small step in the direction that reduces the value of the loss function. The size of this step is determined by a learning rate, which is a hyperparameter that controls the step size. The gradient descent algorithm does not work for all functions. There are two specific requirements. A function has to be: 1) differentiable and 2) convex.\n",
    "It's easy and efficient to implement. However, it has some limitations such as the problem of getting stuck in local minima and the need to tune the learning rate.\n",
    "    - **Batch Gradient Descent**: In this variant, the gradient is calculated using the whole dataset before updating the parameters.\n",
    "    - **Stochastic Gradient Descent (SGD)**: In this variant, the gradient is calculated using a single sample from the dataset at each iteration.\n",
    "    - **Mini-batch Gradient Descent**: In this variant, the gradient is calculated using a small subset (batch) of the dataset at each iteration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "49f043f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "164363a5",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{ciao}}$\n",
    "<span style='color:green'> message/text </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
