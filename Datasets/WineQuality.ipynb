{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality\n",
    "Here, I will apply different ML algorithms to predict quality of wine.\n",
    "\n",
    "1) **Normal equation**\n",
    "\n",
    "The prediction of wine quality by **normal equation** is a **linear regression** task. **Normal equation** is an analytical approach to Linear Regression with a Least Square Cost Function.\n",
    "We can directly find out the value of Î¸ without using Gradient Descent. Following this approach is an effective and time-saving option when are working with a dataset with small features.\n",
    "(click on the link to find more <a href=\"https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression/\"><code>link1</code></a> and <a href=\"http://mlwiki.org/index.php/Normal_Equation\"><code>link2</code></a>\n",
    "\n",
    "2) **Random forest classifier**\n",
    "\n",
    "3) **Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ipcsUFDUzm9C"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get data online:\n",
    "<details>\n",
    "    The code snippet below is responsible for downloading the dataset for example when running via Google Colab. You can also directly download the file using the link if you work with a local setup (in that case, ignore the the block below with !wget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NopU99AT9G7s",
    "outputId": "accea0c2-ecb3-4f22-f035-de9f727b3d43"
   },
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ONqeI5Uzm9H",
    "outputId": "e9e59506-ca1f-43d8-9540-1e38435d0bbf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: (4898, 12)\n"
     ]
    }
   ],
   "source": [
    "# load data from csv file and make a numpy array\n",
    "data = np.genfromtxt('winequality-white.csv',delimiter=\";\",skip_header=1)\n",
    "\n",
    "print(\"data:\", data.shape)\n",
    "\n",
    "# Prepare for proper training\n",
    "np.random.shuffle(data) # randomly sort examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first 3000 examples for training\n",
    "x_train = data[:3000,:11] # all features except last column\n",
    "y_train = data[:3000,11]  # quality column\n",
    "\n",
    "# and the remaining examples for testing\n",
    "x_test = data[3000:,:11]\n",
    "y_test = data[3000:,11]\n",
    "\n",
    "print(\"First example:\")\n",
    "print(\"Features:\", x_train[0]) # [0] refers to the first example\n",
    "print(\"Quality:\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"fixed acidity\", \"volatile acidity\", \"citric acid\", \n",
    "            \"residual sugar\", \"chlorides\", \"free sulfur dioxide\", \n",
    "            \"total sulfur dioxide\", \"density\", \"pH\", \"sulphates\", \"alcohol\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiwnyNHpzm9L"
   },
   "source": [
    "## Data visualization\n",
    "\n",
    "First we want to understand the data better. \n",
    "* Plot (`plt.hist`) the distribution of each of the features for the training data.\n",
    "* the 2D distribution (either `plt.scatter` or `plt.hist2d`) of each feature versus quality.\n",
    "* Also calculate the correlation coefficient (`np.corrcoef`) for each feature with quality. Which feature by itself seems most predictive for the quality?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <a href=\"https://realpython.com/python-enumerate/\"><code>enumerate</code></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "68s7HLT-zm9M",
    "outputId": "9e4efcea-8057-4284-c69d-5a50257217ee",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop over all features\n",
    "for element_index, element in enumerate(features):\n",
    "    \n",
    "    # 1D Histogram \n",
    "    plt.hist(x_train[:,element_index])\n",
    "    plt.xlabel(element)\n",
    "    plt.ylabel(\"Wines\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Scatter plot 1 and 2 show same information in different representations.\n",
    "    # Scatter Plot 1\n",
    "    plt.scatter(x_train[:,element_index],y_train, s=1, alpha=0.2)\n",
    "    plt.xlabel(element)\n",
    "    plt.ylabel(\"Quality\")\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter Plot 2\n",
    "    plt.hist2d(x_train[:,element_index],y_train, bins=[10, np.arange(0.5, 10.5, 1)])\n",
    "    plt.xlabel(element)\n",
    "    plt.ylabel(\"Quality\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Calulate correlation coefficient\n",
    "    plt.clf()\n",
    "    print(f\"Feature: {element}\")\n",
    "    print(f\"Correlation coefficient: {np.corrcoef(x_train[:,element_index],y_train)[0,1]:.3f}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaeDWDuZ0Rnl"
   },
   "source": [
    "##  1) Normal equation\n",
    "\n",
    "* Calculate the linear regression weights by solving the normal equation: \n",
    "     $$ W = (x^T x)^{-1} x^T y$$\n",
    "\n",
    "<details>\n",
    "    \n",
    "    * Numpy provides functions for \n",
    "        * matrix multiplication (`np.matmul`), \n",
    "        * matrix transposition (`.T`),\n",
    "        * matrix inversion (`np.linalg.inv`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-G2ydeLzm9P",
    "outputId": "5f58fa44-ed1e-4cc3-b88c-c36444b3e2e5"
   },
   "outputs": [],
   "source": [
    "# Calulate weights using train data\n",
    "\n",
    "w = np.matmul(np.matmul(np.linalg.inv(np.matmul(x_train.T, x_train)), x_train.T),y_train)\n",
    "\n",
    "print(w)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQIx9onx0Q9k"
   },
   "source": [
    "* Use the weights to predict the quality for the test dataset.\n",
    "    * `y_{predict} = x_{test} w`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "v6LqA03Pzm9R",
    "outputId": "5ec67dc4-0ed7-47a1-a802-035ffeecb0f4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate linear regression model \n",
    "y_pred = np.matmul(x_test,w)\n",
    "\n",
    "print(x_test.shape,w.shape,y_pred.shape)\n",
    "print(x_test[0])\n",
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To find how the predicted quality is good compared to the true quality of the test data, calculate the correlation coefficient between predicted and true quality and draw a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation coefficient:\", np.corrcoef(y_pred,y_test)[0,1])\n",
    "\n",
    "# Prepare scatter plot\n",
    "plt.scatter(y_pred,y_test)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pandas DataFrame of data\n",
    "columns = [\"fixed acidity\", \"volatile acidity\", \"citric acid\", \n",
    "            \"residual sugar\", \"chlorides\", \"free sulfur dioxide\", \n",
    "            \"total sulfur dioxide\", \"density\", \"pH\", \"sulphates\", \"alcohol\", \"quality\"]\n",
    "df = pd.DataFrame(data, columns=columns )\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset does not have any null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=20,figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the quality of wine change by alcohol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "plt.bar(df['quality'],df['alcohol'])\n",
    "plt.xlabel('quality')\n",
    "plt.ylabel('alcohol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation by visualization\n",
    "plt.figure(figsize=[20,10])\n",
    "# plot correlation\n",
    "sb.heatmap(df.corr(),annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.corr(method='pearson',min_periods=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target\n",
    "X = df.drop(['quality'],axis=1)\n",
    "\n",
    "Y = df['quality']\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "X_scaled.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train set into random train and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "# creating train test splits\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_scaled, Y, train_size=0.7, test_size=0.3)\n",
    "\n",
    "print(f\"No. of training examples: {X_train.shape[0]}\")\n",
    "print(f\"No. of testing examples: {X_valid.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pre = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "MSE = mean_squared_error(Y_valid,Y_pre)\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "print('mean squared error is',MSE)\n",
    " \n",
    "print('............................................')\n",
    " \n",
    "print('root mean squared error is',RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Neural network\n",
    "\n",
    "The goal here is the implementation of a simple neural network with one hidden layer using **gradiant descent**.\n",
    "\n",
    "Consider\n",
    "* One hidden layer between input and outpu layers,\n",
    "    * number of features: n_inputs\n",
    "    * number of neurons in hidden layer: hidden_nodes\n",
    "* Activation function,\n",
    "    * reLu activation function\n",
    "\n",
    "$$\\hat{y}=\\mathbf{W}^{\\prime} \\sigma(\\mathbf{W} \\vec{x}+\\vec{b})+b^{\\prime}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization:\n",
    "\n",
    "<details>\n",
    "    Before apply the neural network to get better results we normalize the input data.\n",
    "    We apply normalization manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean value of each columns of data array by axis = 0\n",
    "mean = np.mean(data, axis=0)\n",
    "print(mean.shape)\n",
    "# standard deviation of each columns of data array\n",
    "std = np.std(data, axis = 0)\n",
    "\n",
    "# Normalized data:\n",
    "data = (data - mean)/std\n",
    "\n",
    "\n",
    "# determine train and test sets:\n",
    "X_train = data[:3000,:11]\n",
    "y_train = data[:3000,11]\n",
    "\n",
    "X_test = data[3000:,:11]\n",
    "y_test = data[3000:,11]\n",
    "\n",
    "print(\"First example:\")\n",
    "print(\"Features:\", X_train[0])\n",
    "print(\"Quality:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initialization of weights\n",
    "\n",
    "<details>\n",
    "    Initialise weights with suitable random distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes in the hidden layer\n",
    "hidden_nodes = 50\n",
    "# number of features\n",
    "n_inputs = 11\n",
    "\n",
    "# initialise\n",
    "W = np.random.randn(hidden_nodes,11)*np.sqrt(2./n_inputs)\n",
    "b = np.zeros(hidden_nodes)\n",
    "Wp = np.random.randn(hidden_nodes)*np.sqrt(2./hidden_nodes)\n",
    "bp = np.zeros(1)\n",
    "\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a network\n",
    "* activation function: reLu\n",
    "we do not use written reLu function and write our own.\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(x,W,b,Wp,bp):\n",
    "    return np.dot(Wp,relu(np.dot(W,x)+b))+bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update weights using **gradiant descent**\n",
    "\n",
    "For the **regression** problem the loss function is the **mean squared error** between the prediction and the true label $y$:\n",
    "$$L=(\\hat{y}-y)^{2}$$\n",
    "\n",
    "Taking the partial derivatives - and diligently the applying chain rule - with respect to the different objects yields:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial b^{\\prime}} &=2(\\hat{y}-y) \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_{k}^{\\prime}} &=2(\\hat{y}-y) \\sigma\\left(\\sum_{i} \\mathbf{W}_{i k} x_{i}+b_{k}\\right) \\\\\n",
    "\\frac{\\partial L}{\\partial b_{k}} &=2(\\hat{y}-y) \\mathbf{W}_{k}^{\\prime} \\theta\\left(\\sum_{i} \\mathbf{W}_{i k} x_{i}+b_{k}\\right) \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_{k m}} &=2(\\hat{y}-y) \\mathbf{W}_{m}^{\\prime} \\theta\\left(\\sum_{i} \\mathbf{W}_{i m} x_{i}+b_{m}\\right) x_{k}\n",
    "\\end{aligned}\n",
    "\n",
    "Here, $\\Theta$ denotes the Heaviside step function.\n",
    "Now, update the weights and bias via learning rate $\\alpha$ by\n",
    "\n",
    "\\begin{aligned}\n",
    "b^{\\prime} &= b^{\\prime} - \\alpha \\frac{\\partial L}{\\partial b^{\\prime}} \\\\\n",
    "\\mathbf{W}_{k}^{\\prime} &= \\mathbf{W}_{k}^{\\prime} - \\alpha \\frac{\\partial L}{\\partial \\mathbf{W}_{k}^{\\prime}} \\\\\n",
    "b_{k} &= b_{k} - \\alpha \\frac{\\partial L}{\\partial b_{k}} & \\\\\n",
    "\\mathbf{W}_{k m} &= \\mathbf{W}_{k m} - \\alpha \\frac{\\partial L}{\\partial \\mathbf{W}_{k m}} &\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "<details>\n",
    "    See the links\n",
    "    <a href=\"https://www.tutorialspoint.com/python/python_basic_operators.htm\"><code>python_basic_operators</code></a>\n",
    "    and\n",
    "    <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.outer.html\"><code>outer_product_of_two_vectors</code></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "lr = 0.00005\n",
    "\n",
    "def update_weights(x,y,W,b,Wp,bp):\n",
    "    \n",
    "    z = nn(x,W,b,Wp,bp)\n",
    "    \n",
    "    # Use the formulas derived to calculate the gradient for each of W,b,Wp,bp\n",
    "    delta_bp = 2 * (z-y)\n",
    "    delta_Wp = delta_bp * relu(np.dot(W,x)+b)\n",
    "    delta_b = delta_bp * Wp * np.heaviside(np.dot(W,x)+b,0.5)\n",
    "    delta_W = delta_bp * np.outer(Wp * np.heaviside(np.dot(W,x)+b,0.5),x)\n",
    "    \n",
    "    \n",
    "    # Update the weights/bias following the rule\n",
    "    bp -= lr * delta_bp\n",
    "    Wp -= lr * delta_Wp\n",
    "    b  -= lr * delta_b\n",
    "    W  -= lr * delta_W\n",
    "    \n",
    "    # return new weights and bias\n",
    "    return W, b, Wp, bp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# How many epochs to train\n",
    "n_epochs = 100\n",
    "\n",
    "# Loop over the epochs\n",
    "for ep in range(n_epochs):\n",
    "        \n",
    "    # Each epoch is a complete over the training data\n",
    "    for i in range(X_train.shape[0]):\n",
    "        \n",
    "        # pick one example\n",
    "        x = X_train[i]\n",
    "        y = y_train[i]\n",
    "\n",
    "        # use it to update the weights\n",
    "        W, b, Wp, bp = update_weights(x,y, W, b, Wp, bp)\n",
    "    \n",
    "    # Calculate predictions for the full training and testing sample\n",
    "    y_pred_train = [nn(x,W,b,Wp,bp)[0] for x in X_train]\n",
    "    y_pred = [nn(x,W,b,Wp,bp)[0] for x in X_test]\n",
    "\n",
    "    train_loss = sum((y_pred_train-y_train)**2) / y_train.shape[0]\n",
    "    test_loss = sum((y_pred-y_test)**2) / y_test.shape[0] \n",
    "    \n",
    "    # print some information\n",
    "    print(\"Epoch:\",ep, \"Train Loss:\", train_loss, \"Test Loss:\", test_loss)\n",
    "    \n",
    "    # store the losses for later use\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training:\n",
    "    \n",
    "# Prepare scatter plot\n",
    "y_pred = [nn(x,W,b,Wp,bp)[0] for x in X_test]\n",
    "\n",
    "# now we need to rescale the output to the correct values\n",
    "y_pred = (y_pred + mean[11])* std[11]\n",
    "y_test = (y_test + mean[11])* std[11]\n",
    "y_pred_train = (y_pred_train + mean[11]) * std[11]\n",
    "y_train = (y_train + mean[11]) * std[11]\n",
    "\n",
    "\n",
    "print(f\"Best loss: {min(test_losses):.3f}, Final loss: {test_losses[-1]:.3f}\")\n",
    "\n",
    "print(\"Correlation coefficient:\", np.corrcoef(y_pred,y_test)[0,1])\n",
    "plt.scatter(y_pred_train,y_train)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Prepare and loss over time\n",
    "plt.plot(train_losses[2:],label=\"train\")\n",
    "plt.plot(test_losses[2:],label=\"test\") # we omit the first data points as the first loss is typically very high which makes it difficult to read the plot. \n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Neural network by Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('winequality-white.csv',delimiter=\";\",skip_header=1)\n",
    "\n",
    "print(\"data:\", data.shape)\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Normalize\n",
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis = 0)\n",
    "print(mean.shape)\n",
    "\n",
    "data = (data - mean)/std\n",
    "\n",
    "X_train = data[:3000,:11]\n",
    "y_train = data[:3000,11] \n",
    "\n",
    "X_test = data[3000:,:11]\n",
    "y_test = data[3000:,11]\n",
    "\n",
    "print(\"First example:\")\n",
    "print(\"Features:\", X_train.shape)\n",
    "print(\"Quality:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_nodes = 50\n",
    "input = X_train.shape[(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "                          layers.Dense(units=hidden_nodes, activation='relu', input_shape=[input]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mae\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=256,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert the training history to a dataframe\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# use Pandas native plot method\n",
    "history_df['loss'].plot();\n",
    "history_df['val_loss'].plot();"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WineQuality_NormalEquation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
