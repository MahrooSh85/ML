{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484de862",
   "metadata": {},
   "source": [
    "**Metrics** are used to evaluate the model we use to train the data and use it for prediction of unseen data.\n",
    "\n",
    "* **Classification** metrics:\n",
    "    - Confusion matrix: A confusion matrix is a summary of the predictions made by a classification model organized into a table by class. Each row of the table indicates the actual class and each column represents the predicted class. The confusion matrix provides more insight into not only the accuracy of a predictive model, but also which classes are being predicted correctly, which incorrectly, and what type of errors are being made. The simplest confusion matrix is for a two-class classification problem, with negative (class 0) and positive (class 1) classes.\n",
    "        - True positives (TP): you predict an observation belongs to a class and it actually does belong to that class.\n",
    "        - True negatives (TN): you predict an observation does not belong to a class and it actually does not belong to that class.\n",
    "        - False positives (FP): you predict an observation belongs to a class when in reality it does not.\n",
    "        - False negatives (FN): you predict an observation does not belong to a class when in fact it does.\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "* **Regression** metrics:\n",
    "    - R2_score\n",
    "    - mean_absolute_error\n",
    "    - mean_squared_error\n",
    "    - median_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023ea12",
   "metadata": {},
   "source": [
    "- Accuracy:\n",
    "    - the percentage of correct predictions.\n",
    "    * $\\frac{TN+TP}{TN+TP+FN+FP}$\n",
    "- Classification Error: \n",
    "    - 1 - Accuracy\n",
    "    - $\\frac{FN+FP}{TN+TP+FN+FP}$\n",
    "- Recall\n",
    "    - the fraction of examples which were predicted to belong to a class with respect to all of the examples that truly belong in the class.\n",
    "    - $\\frac{TP}{TP+FN}$\n",
    "    - High score of true positive but also avoided false negatives (like cancer tomour)\n",
    "- Precision\n",
    "    - the fraction of relevant examples (true positives) among all of the examples which were predicted to belong in a certain class.\n",
    "    - $\\frac{TP}{TP+FP}$\n",
    "    - avoid false positives\n",
    "    \n",
    "Precision and recall are useful in cases where classes aren't evenly distributed.\n",
    "\n",
    "The $\\beta$ parameter allows us to control the tradeoff of importance between precision and recall. $\\beta<1$ focuses more on precision while $\\beta>1$ focuses more on recall.\n",
    "\n",
    "- False positive rate (FPR)\n",
    "    - What fraction of all negative instances does the classifier incorrectly identify as positive\n",
    "    - $\\frac{FP}{TN+FP}$\n",
    "- Precision-Recall tradeoff\n",
    "    - $F_1 = 2 \\frac{Precision*Recall}{Precision+Recall} = \\frac{2 TP}{2 TP+FN+FP}$\n",
    "- F-score\n",
    "    - Generalizes $F_1$ score\n",
    "    - $F_\\beta = (1+\\beta^2) \\frac{Precision*Recall}{\\beta^2 Precision+Recall} = \\frac{(1+\\beta^2) TP}{(1+\\beta^2) TP+\\beta FN+FP}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a09302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
