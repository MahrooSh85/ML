{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c01fb4",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484de862",
   "metadata": {},
   "source": [
    "**Metrics** are used to evaluate the model we use to train the data and use it for prediction of unseen data.\n",
    "\n",
    "* **Classification** metrics:\n",
    "    - Confusion matrix: A confusion matrix is a summary of the predictions made by a classification model organized into a table by class. Each row of the table indicates the actual class and each column represents the predicted class. The confusion matrix provides more insight into not only the accuracy of a predictive model, but also which classes are being predicted correctly, which incorrectly, and what type of errors are being made. The simplest confusion matrix is for a two-class classification problem, with negative (class 0) and positive (class 1) classes.\n",
    "        - True positives (TP): you predict an observation belongs to a class and it actually does belong to that class.\n",
    "        - True negatives (TN): you predict an observation does not belong to a class and it actually does not belong to that class.\n",
    "        - False positives (FP): you predict an observation belongs to a class when in reality it does not.\n",
    "        - False negatives (FN): you predict an observation does not belong to a class when in fact it does.\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "* **Regression** metrics:\n",
    "    - R2_score\n",
    "    - mean_absolute_error\n",
    "    - mean_squared_error\n",
    "    - median_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023ea12",
   "metadata": {},
   "source": [
    "What we can extrcat from **Confusion matrix**:\n",
    "\n",
    "- <span style='color:purple'> Accuracy: </span>\n",
    "    - $\\frac{TN+TP}{TN+TP+FN+FP}$\n",
    "    \n",
    "    - the percentage of correct predictions.\n",
    "    \n",
    "    \n",
    "    \n",
    "- <span style='color:purple'> Classification Error: </span>\n",
    "    \n",
    "    - $\\frac{FN+FP}{TN+TP+FN+FP}$\n",
    "    \n",
    "    - 1 - Accuracy\n",
    "\n",
    "\n",
    "- <span style='color:purple'> False positive rate (FPR) </span>\n",
    "    - $\\frac{FP}{TN+FP}$\n",
    "    \n",
    "    - What fraction of all negative instances does the classifier incorrectly identify as positive.\n",
    "\n",
    "    \n",
    "- <span style='color:purple'> Recall </span>\n",
    "    - $\\frac{TP}{TP+FN}$\n",
    "    \n",
    "    - Also known as **sensitivity** or **True Positive Rate**\n",
    "    \n",
    "    - The fraction of examples which were predicted to belong to a class with respect to all of the examples that truly belong in the class.\n",
    "    - It gives us high TP and minimize FN. When we do not want to loose any positive result even with the cost of having more FP. High score of true positive but also avoided false negatives (like cancer tomour)\n",
    "    \n",
    "    \n",
    "- <span style='color:purple'> Precision </span>\n",
    "    - $\\frac{TP}{TP+FP}$\n",
    "    \n",
    "    - The fraction of relevant examples (true positives) among all of the examples which were predicted to belong in a certain class.\n",
    "    \n",
    "    - Avoid false positives\n",
    "    \n",
    "Precision and recall are useful in cases where classes aren't evenly distributed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "- <span style='color:purple'> Precision-Recall tradeoff </span>\n",
    "    - $F_1 = 2 \\frac{Precision*Recall}{Precision+Recall} = \\frac{2 TP}{2 TP+FN+FP}$\n",
    "    \n",
    "    - In general, there is a tradeoff between recall and precision (increasing one will decrease another). So we should see the problem and see which one is more important.\n",
    "        * Examples of recall-oriented tasks: - info. extraction in legal discovery - Tumor detection - Often should be paird with human expert to filter out FP.\n",
    "        * Examples of precision-oriented tasks: - Search engine ranking, query suggestion - Documentation classification - Customer-facing tasks.\n",
    "    \n",
    "    \n",
    "- <span style='color:purple'> F-score </span>\n",
    "    \n",
    "    - $F_\\beta = (1+\\beta^2) \\frac{Precision*Recall}{\\beta^2 Precision+Recall} = \\frac{(1+\\beta^2) TP}{(1+\\beta^2) TP+\\beta FN+FP}$\n",
    "    \n",
    "    - Generalizes $F_1$ score\n",
    "\n",
    "The $\\beta$ parameter allows us to control the tradeoff of importance between precision and recall. $\\beta<1$ focuses more on precision while $\\beta>1$ focuses more on recall.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To access to some of the scores:\n",
    "### <span style='color:green'>scikit-learn:</span>\n",
    "\n",
    "classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e130c",
   "metadata": {},
   "source": [
    "### Classifier decision function\n",
    "\n",
    "- Many classifier has the option to tell us about the uncertainity in the prediction by methods,\n",
    "    * **predict_proba**\n",
    "    * **decision_function**\n",
    "    \n",
    "### <span style='color:green'>scikit-learn:</span>\n",
    "\n",
    "- predict_proba:\n",
    "    * returns the class probabilities for each data point, a score between (0,1).\n",
    "- decision_function:\n",
    "    * returns the distance between the hyperplane and the test instance\n",
    "    * Predict confidence scores for samples, a score between (-1,1).    \n",
    "\n",
    "Moreover, we can sweep the decision threshold (based on the problem) and tune our metric. (for example we may want not to loose any positive class, so we put the threshold into a low value, even with the risk of decreasing the precision.) Then, we would have the classification outcome as a curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23182f",
   "metadata": {},
   "source": [
    "## Precision-Recall curves\n",
    "\n",
    "- X-axis: Precision\n",
    "- Y-axis: Recall\n",
    "- The goal is to maximize precision while maximizing recall.\n",
    "    * Top right corner where both P, and R are 1 is the ideal point.\n",
    "\n",
    "\n",
    "### <span style='color:green'>scikit-learn:</span>\n",
    "- sklearn.metrics.precision_recall_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f021c4b",
   "metadata": {},
   "source": [
    "\n",
    "## ROC (Receiving operating characteristic) curves\n",
    "\n",
    "- X-axis: False positive rate\n",
    "- Y-axis: True positive rate\n",
    "- We want a curve that maximize the TPR and minimize the FPR.\n",
    "    * Top left is the ideal point where the FP rate is zero and TP rate is one.\n",
    "- In a random classifier's ROC curve is a line on the diagonal. So a bad classifier's curve is under that line and a good one is far above that line.\n",
    "- This implementation is restricted to the binary classification task.\n",
    "\n",
    "### <span style='color:green'>scikit-learn:</span>\n",
    "* sklearn.metrics.roc_curve\n",
    "\n",
    "## AUC (Area under the curve)\n",
    "It is area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores. For a random classifier it is 0.5 and as the classifier becomes better, it approaches 1.\n",
    "\n",
    "### <span style='color:green'>scikit-learn:</span>\n",
    "- sklearn.metrics.roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd930a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
