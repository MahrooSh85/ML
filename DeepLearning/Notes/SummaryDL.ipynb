{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6547a763",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c332bc7",
   "metadata": {},
   "source": [
    "**Tensorflow** is an end-to-end open source platform for machine learning.\n",
    "\n",
    "There are five steps\n",
    "\n",
    "1- Define the model,\n",
    "    \n",
    "    - layers,\n",
    "\n",
    "2- Compile the model,\n",
    "    \n",
    "    - optimizer,\n",
    "    - loss function,\n",
    "    - metric,\n",
    "    - model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "3- Fit the model,\n",
    "\n",
    "    - model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=100, batch_size=32)\n",
    "\n",
    "4- Evaluate the model,\n",
    "\n",
    "    - model.evaluate(X_validation, y_validation, verbose=0)\n",
    "\n",
    "5- Make predictions,\n",
    "\n",
    "    - model.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b9eb6",
   "metadata": {},
   "source": [
    "In neural network model, the activation function, loss function and optimization algorithm play a very important role in efficiently and effectively training a model and produce accurate results.\n",
    "\n",
    "### Optimizer:\n",
    "\n",
    "Optimizers are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the losses. Optimizer minimizes the loss function through **back propagation**. It can tell the network how to change its weights.\n",
    "Optimisation functions usually calculate the gradient i.e. the partial derivative of loss function with respect to weights, and the weights are modified in the opposite direction of the calculated gradient. This cycle is repeated until we reach the minima of loss function.\n",
    "\n",
    "- **adam**,\n",
    "- **Stochastic Gradient Decent (sgd)**,\n",
    "    * The _gradient_ is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change fastest. We call our process gradient _descent_ because it uses the gradient to descend the loss curve towards a minimum. _Stochastic_ means \"determined by chance.\" Our training is stochastic because the minibatches are random samples from the dataset. And that's why it's called SGD!\n",
    "\n",
    "\n",
    "\n",
    "### Loss function:\n",
    "Loss function shows difference between output and target variable. It measures how good the network's predictions are. The three most common loss functions are:\n",
    "\n",
    "- **binary_crossentropy** for binary classification,\n",
    "- **sparse_categorical_crossentropy** for multi-class classification,\n",
    "- **mse** (mean squared error) for regression,\n",
    "\n",
    "\n",
    "#### Keywords:\n",
    "\n",
    "- **epochs**: loops through the training dataset. The number of epochs you train for is how many times the network will see each training example.\n",
    "\n",
    "- **batch_size**: the number of samples in an epoch used to estimate model error. Each iteration's sample of training data is called a minibatch (or often just \"batch\")\n",
    "\n",
    "- **verbose**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 1)\n",
    "# no hidden layer\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=number_of_outputs, \n",
    "                 input_shape=[number_of_inputs])\n",
    "])\n",
    "\n",
    "# 2)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mae\",\n",
    ")\n",
    "\n",
    "# 3)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=256,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "## Find out the loss function during iteration: (read the box below)\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "history_df['loss'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd8f9d8",
   "metadata": {},
   "source": [
    "The fit method in fact keeps a record of the training and validation loss produced during training in a History object. It's better to convert the data to a Pandas dataframe, which makes the plotting easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd134c",
   "metadata": {},
   "source": [
    "# Neural network:\n",
    "\n",
    "### Single neuron:\n",
    "The fundamental component of neural network is a **linear unit** or a single **neuron** with one input ($x$) where $y = w.x + b$ . The input ($x$) is connected to neuron by weight  ($w$). The bias ($b$) enables the neuron to modify the output independently of its inputs.\n",
    "\n",
    "### Layers:\n",
    "Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a dense layer.\n",
    "There are different types of layers in Keras:\n",
    "- Convolutional\n",
    "- recurrent\n",
    "\n",
    "The layers before the output layer are sometimes called hidden since we never see their outputs directly.\n",
    "\n",
    "To improve the model we connected layers together through an **activation functio**. The activation function is applied to each of a layer's outputs (its activations). The most common is the rectifier function.\n",
    "- ReLU ()\n",
    "    - (0, infinity)\n",
    "- Leaky ReLU\n",
    "- Sigmoid or Logistic Activation Function\n",
    "    - S-shape between (0,1)\n",
    "    - It is especially used for models where we have to predict the probability as an output since probability is a number between the range of 0 and 1.\n",
    "- Softmax function\n",
    "    - more generalized logistic activation function which is used for multiclass classification.\n",
    "- Tanh\n",
    "    - S-shape between (-1,1)\n",
    "    - tanh is also like logistic sigmoid but better.\n",
    "\n",
    "Depending on our task, activation function can be applied in the output layer or not. No activation function makes the network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5789b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units= number_of_outputs_1th_layer, activation='activation_function', input_shape=[number_of_inputs]),\n",
    "    layers.Dense(units= number_of_outputs_2nd_layer, activation='activation_function'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dabbd4",
   "metadata": {},
   "source": [
    "## Learning rate and minibatchs\n",
    "\n",
    "The **learning rate** is a tuning parameter in an **optimization** algorithm that determines the step size at each iteration while moving toward a **minimum** of a loss function.\n",
    "\n",
    "A **smaller** learning rate means the network needs to see **more** minibatches before its weights converge to their best values.\n",
    "\n",
    "The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn't always obvious.\n",
    "\n",
    "Fortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. **Adam** is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c435b",
   "metadata": {},
   "source": [
    "### Capacity\n",
    "\n",
    "Information in the training data as being of two kinds: _signal_ and _noise_. The _signal_ is the part that _generalizes_, the part that can help our model make predictions from new data. The _noise_ is that part that is _only_ true of the training data; the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model make predictions.\n",
    "\n",
    "To get more signal out of the training data while reducing the amount of noise, capacity of a model should be considered.\n",
    "\n",
    "A model's **capacity** refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.\n",
    "\n",
    "You can increase the capacity of a network either by making it **wider** (more units to existing layers) or by making it **deeper** (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de82a0",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "**Early stopping** is a form of **regularization** used to avoid overfitting when training a learner with an iterative method, such as gradient descent. \n",
    "\n",
    "When a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn't decreasing anymore. Interrupting the training this way is called early stopping. Besides preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5675a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c318fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6d551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d5b8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=[11]),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=256,\n",
    "    epochs=500,\n",
    "    callbacks=[early_stopping], # put your callbacks in a list\n",
    "    verbose=0,  # turn off training log\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
