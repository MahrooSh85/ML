{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16c4a6b",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6958b3",
   "metadata": {},
   "source": [
    "**Tensorflow** is an end-to-end open source platform for machine learning.\n",
    "\n",
    "There is five steps\n",
    "\n",
    "1- Define the model,\n",
    "    \n",
    "    - layers,\n",
    "\n",
    "2- Compile the model,\n",
    "    \n",
    "    - optimizer,\n",
    "    - loss function,\n",
    "    - metric,\n",
    "    - model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "3- Fit the model,\n",
    "\n",
    "    - model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "4- Evaluate the model,\n",
    "\n",
    "    - model.evaluate(X_validation, y_validation, verbose=0)\n",
    "\n",
    "5- Make predictions,\n",
    "\n",
    "    - model.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d11f5",
   "metadata": {},
   "source": [
    "In neural network model, the activation function, loss function and optimization algorithm play a very important role in efficiently and effectively training a model and produce accurate results.\n",
    "\n",
    "### Optimizer:\n",
    "\n",
    "Optimizers are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the losses. Optimizer minimizes the loss function through **back propagation**.\n",
    "Optimisation functions usually calculate the gradient i.e. the partial derivative of loss function with respect to weights, and the weights are modified in the opposite direction of the calculated gradient. This cycle is repeated until we reach the minima of loss function.\n",
    "\n",
    "- **adam**,\n",
    "- **Stochastic Gradient Decent (sgd)**,\n",
    "\n",
    "\n",
    "\n",
    "### Loss function:\n",
    "Loss function shows difference between output and target variable. The three most common loss functions are:\n",
    "\n",
    "- **binary_crossentropy** for binary classification,\n",
    "- **sparse_categorical_crossentropy** for multi-class classification,\n",
    "- **mse** (mean squared error) for regression,\n",
    "\n",
    "\n",
    "#### Keywords:\n",
    "\n",
    "- **epochs**: loops through the training dataset\n",
    "\n",
    "- **batch_size**: the number of samples in an epoch used to estimate model error\n",
    "\n",
    "- **verbose**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# no hidden layer\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=number_of_outputs, input_shape=[number_of_inputs])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5a2b0",
   "metadata": {},
   "source": [
    "# Neural network:\n",
    "\n",
    "### Single neuron:\n",
    "The fundamental component of neural network is a **linear unit** or a single **neuron** with one input ($x$) where $y = w.x + b$ . The input ($x$) is connected to neuron by weight  ($w$). The bias ($b$) enables the neuron to modify the output independently of its inputs.\n",
    "\n",
    "### Layers:\n",
    "Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a dense layer.\n",
    "There are different types of layers in Keras:\n",
    "- Convolutional\n",
    "- recurrent\n",
    "\n",
    "The layers before the output layer are sometimes called hidden since we never see their outputs directly.\n",
    "\n",
    "To improve the model we connected layers together through an **activation functio**. The activation function is applied to each of a layer's outputs (its activations). The most common is the rectifier function.\n",
    "- ReLU ()\n",
    "    - (0, infinity)\n",
    "- Leaky ReLU\n",
    "- Sigmoid or Logistic Activation Function\n",
    "    - S-shape between (0,1)\n",
    "    - It is especially used for models where we have to predict the probability as an output since probability is a number between the range of 0 and 1.\n",
    "- Softmax function\n",
    "    - more generalized logistic activation function which is used for multiclass classification.\n",
    "- Tanh\n",
    "    - S-shape between (-1,1)\n",
    "    - tanh is also like logistic sigmoid but better.\n",
    "\n",
    "Depending on our task, activation function can be applied in the output layer or not. No activation function makes the network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be943fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units= number_of_outputs_1th_layer, activation='activation_function', input_shape=[number_of_inputs]),\n",
    "    layers.Dense(units= number_of_outputs_2nd_layer, activation='activation_function'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
