{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7d82f3",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> K-nearest neighbor (KNN) </span>\n",
    "* A **non-parametric** model\n",
    "* A **non-linear** model\n",
    "\n",
    "It makes few assumptions about structure of data and usually gives accurate result, but it is unstable to small changes in the dataset.\n",
    "\n",
    "* Classifier\n",
    "* Regressor\n",
    "\n",
    "Instance based or memory based supervised learning.\n",
    "\n",
    "- KNN classifier: memorize the entire training set.\n",
    "\n",
    "\n",
    "Four things should be specified:\n",
    "\n",
    "    1) A distance metric.\n",
    "        * it controls the distance function between points and thus which points are considered as nearest in finding neighbors.\n",
    "        * Typically Euclidean (Minkowski with p = 2)\n",
    "    2) How many nearest neighbors to look at? (Model complexity)\n",
    "        * k=5\n",
    "    3) Optional weighting function on the neighbor points.\n",
    "        * Ignored\n",
    "    4) Methods for aggregating the classes of neighbor points.\n",
    "        * Majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca618c9",
   "metadata": {},
   "source": [
    "### Relation between $k$ and model complexity\n",
    "\n",
    "* **Reducing $k$** in knn classifier **increases** the variance of the decision boundries and the risk of **Overfitting** because very local changes is captured.\n",
    "\n",
    "* **$k=$the total number** of points in the training set, the result would be a **single decision** which it is the **most frequent** calss in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d319428",
   "metadata": {},
   "source": [
    "### Drawback:\n",
    "\n",
    "When the training data has many samples, or each sample has lots of features, this can slow down the performance of KNN model.\n",
    "\n",
    "For data set with hundred of thousands of features, especially if it is sparse, we should apply another model in stead of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fdb4c4",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>  Linear regression: </span>\n",
    "\n",
    "A **parametric** model. It assumes a linear relationship between the input variables (features) and the single output variable (target).\n",
    "\n",
    " It gives the target based on weighted sum of the features. The task of machine learning is to find the weighting parameters based on the previous data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36427ac0",
   "metadata": {},
   "source": [
    "**Least square linear regression** (AKA: ordinary least square)\n",
    "\n",
    "* It minimize the mean square error between target and prediction to find ws(weights) and b (bias/intercept parameter)\n",
    "\n",
    " $RSS(w,b) = \\sum_{i=1}^N (y_i - (w.x_i + b))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a6959b",
   "metadata": {},
   "source": [
    "### <span style='color:green'>scikit-learn</span>:\n",
    "\n",
    "* $w$ : linreg.coef_\n",
    "\n",
    "* $b$: linreg.intercept_\n",
    "\n",
    "    -The ' _ ' in linreg.coef_ means it is a parameter that has been derived by training the data and it is not set by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378cc17c",
   "metadata": {},
   "source": [
    "## Comparing between KNN and Linear regression:\n",
    "\n",
    "* **KNN**:\n",
    "    - does not make a lot of assumption about the structure of the data.\n",
    "    - gives potentially accurate but sometimes unstable predictions that are sensitive to small changes in the training data.\n",
    "    - better on training set.\n",
    "\n",
    "* **Linear regression**:\n",
    "    - makes strong assuptions about the structure of the data: linear relationship.\n",
    "    - gives stable but potentially inaccurate predictions.\n",
    "    - better on unseen data.\n",
    "    - very extendable to new data beyond the training set.\n",
    "    - no parameter to control the complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d4049",
   "metadata": {},
   "source": [
    "## <span style='color:orange'>  Regularization: </span>\n",
    "Regularization prevents **overfitting** by restricting the model typically to reduce its complexity. \n",
    "\n",
    "* Ridge regression (L2)\n",
    "* Lasso regression (L1)\n",
    "\n",
    "### Ridge regression:\n",
    "\n",
    "Using same least-square criterion but adds a penalty for **large variations** in weight parameters.\n",
    "\n",
    "$RSS_{ridge}(w,b) = \\sum_{i=1}^N (y_i - (w.x_i+b))^2 + \\alpha \\sum_{i=1}^P w_j^2$\n",
    "\n",
    "**Higher** $\\alpha$ means **more** regularization and **simpler** models.\n",
    "\n",
    "### Lasso regression:\n",
    "\n",
    "Like Ridge regression, a regullarization penalty term to the ordinary RSS that cause w coefficients to shrink toward zero.\n",
    "\n",
    "$RSS_{lasso}(w,b) = \\sum_{i=1}^N (y_i - (w.x_i+b))^2 + \\alpha \\sum_{i=1}^P |w_j|$\n",
    "\n",
    "With lasso, a subset of the coefficients are forced to be precisely zero. (it is called sparse solution which is a kind of **Feature selection**)\n",
    "\n",
    "By default $\\alpha=0$.\n",
    "\n",
    "\n",
    "### Use\n",
    "* **Ridge**: Many small/Medium sized effects.\n",
    "* **Lasso**: Only a few variables with medium effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6ec26",
   "metadata": {},
   "source": [
    "# Polynomial Features:\n",
    "\n",
    "Generate polynomial and interaction features.\n",
    "\n",
    "* It is still a **linear** model.\n",
    "* Polynomial feature expansion is often combined with a regularization learning method like ridge regression.\n",
    "* Using higher degrees leads to more complex models and regularization might be needed to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2f36b",
   "metadata": {},
   "source": [
    "# Linear model for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab90293c",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>  Logistic regression </span>\n",
    "\n",
    "* Linear model\n",
    "* default: Binary classification but can be applied on multi-class \n",
    "* Applying logistic function (activation function) on estimated probabilities determines the class\n",
    "* Parameter $C$ controls **regularization**\n",
    "    - default: $C = 1$ Ridge (L2) regularization\n",
    "    - **Higher** $C$ corresponds to **less** regularization\n",
    "* Normalization woud be important here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad58a2f",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>  Support vector machine (SVM): </span>\n",
    "\n",
    "* Apply **sign function** as activation function to produce binary output\n",
    "    -feature vector -> linear function: $Sign(w.x+b)$ -> class value\n",
    "* **Classifier margin** is defined as the width the decision boundary area can be increased before hitting a data point.\n",
    "* The **best** classifier has the **maximum** margin.\n",
    "* The **maximum** margin classifier is called the **linear support vector machine (LSVM)**\n",
    "* Parameter $C$ controls **regularization**\n",
    "    - default: $C = 1$ Ridge (L2) regularization\n",
    "    - **Higher** $C$ corresponds to **less** regularization.\n",
    "        * Fit the training data as well as possible\n",
    "        * Each individual data point is important to classify correctly.\n",
    "        \n",
    "* **Drawback**:\n",
    "    \n",
    "\n",
    "* **Benefit**:\n",
    "    - Simple and easy to train\n",
    "    - Fast prediction\n",
    "    - Scales well to very large datasets\n",
    "    \n",
    "### <span style='color:green'>scikit-learn</span>:\n",
    "\n",
    "* sklearn.svm.SVC\n",
    "* sklearn.svm.LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5126e17",
   "metadata": {},
   "source": [
    "## Multiclass classifier:\n",
    "\n",
    "* scikit-learn make multi-class problem into binary problems (one class against all other classes).\n",
    "    - choose the class with **highest score** as the **predicted class**.\n",
    "    - model.coef_ gives us $n$ (number of classes) sets of parameters related to each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b0300",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>  Kernelized Support vector machine (KSVM): </span>\n",
    "\n",
    "????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc45f35",
   "metadata": {},
   "source": [
    "# <span style='color:orange'>  Cross validation: </span>\n",
    "\n",
    "How well a classifier generalizes.\n",
    "\n",
    "* **why?** The test set represented data that had not been seen during training but had the **same** general attributes as the original data set. The normal evaluation on a single test set has a problem: maybe the result becomes good for that specific testset by chance. So we go for **cross-validation**.\n",
    "\n",
    "* **k-fold cross validation**:\n",
    "cross-validation split the dataset to e.g. k=5 parts then consider 1 part as the testset and the rest as training set and evaluate. Then consider the next part as the test set, etc. At the end we can look into the average result.\n",
    "\n",
    "* The **Stratified Cross-validation** means that when splitting the data, the proportions of classes in each fold are made as close as possible to the actual proportions of the classes in the overall data set as shown here.\n",
    "\n",
    "\n",
    "It has two steps:\n",
    "\n",
    "1) Splitting the data set into subsets (k folds):\n",
    "\n",
    "    - Each fold has approximately the same size\n",
    "    - Data can be randomly selected in each fold or stratified\n",
    "\n",
    "2) Rotating, training and validating among them\n",
    "    \n",
    "    - All folds are used to train the model except one, which is used for validation.\n",
    "    - That validation fold should be rotated until all folds have become a validation fold once and only once.\n",
    "    - Each example is recommended to be contained in one and only one fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f043f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "164363a5",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{ciao}}$\n",
    "<span style='color:green'> message/text </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85fd93",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> Decision Tree </span>\n",
    "\n",
    "* Classifier\n",
    "* regression\n",
    "    - the decision tree works like a classifier and then gives the mean value of the result class.\n",
    "\n",
    "Based on a series of **if-then** / **yes-no** questions. Each question is a node. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Different types of nodes:\n",
    "\n",
    "    1) Root node\n",
    "        - Top\n",
    "    2) Decision node\n",
    "        - Middle\n",
    "    3) Terminated node (Leaf)\n",
    "        - bottom\n",
    "\n",
    "### Deawback:\n",
    "\n",
    "**Overfitting** are likely to happen.\n",
    "To prevent overfitting, two additional strategies are used:\n",
    "* pre-pruning\n",
    "    - early stop of growing tree\n",
    "* post-pruning\n",
    "    - ???\n",
    "    \n",
    "### Benefit:\n",
    "* Works well with different types of features (Categorical non-categorical)\n",
    "* No need for normalization\n",
    "    \n",
    "## <span style='color:green'>scikit-learn</span>:\n",
    "\n",
    "### Key parameters: \n",
    "Controling one of the below parameters is enough to control overfitting\n",
    "\n",
    "* **max_depth**: controls maximum number of split points.\n",
    "    - Most common way to reduce the complexity of the model.\n",
    "* **min_samples_leaf**: threshold for minimum number of data instances a leaf can have to avoid further splitting.\n",
    "* **max_leaf_nodes**: limit total number of leaves in the tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e534bb",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> Random forest </span>\n",
    "\n",
    "To avoid **Overfitting** in Decision tree, it is needed to use an ensemble of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b6f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
